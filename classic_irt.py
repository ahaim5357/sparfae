"""
This python module implements 'classic' item response theory,
where the ability parameter is not learned but deterministically
generated by just counting the number of correct answers for
each skill. Then, we only learn the difficulty parameters.

"""

# Sparse Factor Autoencoders for Item Response Theory
# Copyright (C) 2021-2022
# Benjamin Paaßen
# German Research Center for Artificial Intelligence
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

__author__ = 'Benjamin Paaßen'
__copyright__ = 'Copyright 2021-2022, Benjamin Paaßen'
__license__ = 'GPLv3'
__version__ = '0.1.0'
__maintainer__ = 'Benjamin Paaßen'
__email__  = 'benjamin.paassen@dfki.de'

import numpy as np
from scipy.sparse import csr_matrix
from sklearn.base import BaseEstimator
from sklearn.linear_model import LogisticRegression

class ClassicIRT(BaseEstimator):
    """ An item response theory model. This model describes
    the probability that student i gets question j right
    a logistic function of the difference between task difficulty
    and student ability.

    In this particular model, we do not learn the student's ability
    parameter but compute it deterministically as the sum of
    correct answers for a certain skill, i.e.

    theta[i] = np.dot(Q.T, Y[i, :])

    where Q is a matrix that describes the mapping from skills to
    tasks and where Y[i, :] is a vector of the student's answers for
    each item.

    The 'task-relevant' ability for item j is, then, the product of
    theta[i] with Q[j, :]. In more detail, our model is:

    p[i, j] = 1 / (1 + np.exp(a[j] * (b[j] - np.dot(Q[j, :], theta[i]))))

    where b[j] is the difficulty of item j and a[j] models how well
    item j discriminates between students who have the required ability
    and students who do not.

    Still, this model is a logistic regression model, which can be
    re-written in sparse matrix format. For more details, please refer
    to the code below.

    Parameters
    ----------
    Q: ndarray
        A matrix where each row represents a task and each column
        a skill/knowledge component.
        The matrix must be binary, where a q[j, k] = 1 indicates
        that skill k is needed for task j.
        Further, each row is only permitted a single one.
    C: float (default = 1.)
        The regularization parameter for the logistic regression.

    Attributes
    ----------
    a_: ndarray
        The discrimination parameter for each task.
    b_: ndarray
        The difficulty parameter for each task.

    """
    def __init__(self, Q, C = 1.):
        self.Q_ = Q
        self.C = C

    def fit(self, X, Y = None):
        """ Fits a model to the given response data.

        Parameters
        ----------
        X: ndarray
            A matrix of test responses where each row represents
            a student and each column represents a question.
        Y: ndarray (default = None)
            Not needed. Only here for consistency with sklearn
            interface.

        Returns
        -------
        self

        """
        # check that each entry of the Q matrix is zero or 1
        if np.any(np.abs(np.abs(self.Q_) + np.abs(self.Q_ - 1) - 1) > 1E-3):
            raise ValueError('The Q Matrix needs to be binary.')
        # check that each row contains exactly one one
        if np.any(np.abs(1 - np.sum(self.Q_, 1)) > 1E-3):
            raise ValueError('Each row in the Q matrix needs to contain exactly a single one.')
        # check that the dimensions fit with the training data
        if X.shape[1] != self.Q_.shape[0]:
            raise ValueError('Expected one column in X for each row in Q.')

        num_tasks = self.Q_.shape[0]

        # replace nan entries with task-specific mean success rate for ability
        # computation
        self.p_ = np.nanmean(X, 0)
        Xinterp = np.copy(X)
        for j in range(num_tasks):
            nansj = np.isnan(X[:, j])
            Xinterp[nansj, j] = self.p_[j]

        # compute the task-relevant knowledge for each student on each task
        Theta  = np.dot(Xinterp, self.Q_)
        Theta2 = np.dot(Theta, self.Q_.T)

        # for each task, we run a separate optimization to fit the difficulty and
        # discrimination parameter
        self.a_ = np.zeros(num_tasks)
        self.b_ = np.zeros(num_tasks)
        for j in range(num_tasks):
            # check if the success rate is 100% or 0%. In these special cases, we
            # do not need to train a model
            if self.p_[j] >= 1.:
                self.a_[j] = 1.
                self.b_[j] = -100
                continue
            if self.p_[j] <= 0.:
                self.a_[j] = 1.
                self.b_[j] = 100*np.max(Theta2[:, j])
                continue
            # remove nans
            nansj = np.isnan(X[:, j])
            if np.any(nansj):
                nonnansj = np.logical_not(nansj)
                thetasj  = Theta2[nonnansj, j]
                yj       = X[nonnansj, j]
            else:
                thetasj  = Theta2[:, j]
                yj       = X[:, j]

            # the logistic regression for task j tries to predict the correctness
            # of a response from task-specific knowledge as single input feature.
            # The weight is the discrimination, the intercept corresponds to task difficulty
            model_j = LogisticRegression(C = self.C)
            model_j.fit(np.expand_dims(thetasj, 1), yj)
            # extract the parameters
            self.a_[j] = model_j.coef_[0, 0]
            self.b_[j] = -model_j.intercept_[0] / self.a_[j]

        return self


    def encode(self, X):
        """ Computes the predicted knowledge of each student.

        This is just np.dot(X, self.Q_).

        Parameters
        ----------
        X: ndarray
            A matrix of test responses where each row represents
            a student and each column represents a question.

        Returns
        -------
        Theta: ndarray
            A matrix of predicted knowledge for each student, where
            each row represents a student and each column represents
            a skill.

        """
        # remove nans
        nans = np.isnan(X)
        if np.any(nans):
            X = np.copy(X)
            for j in range(len(self.p_)):
                X[nans[:, j], j] = self.p_[j]
        # encode
        return np.dot(X, self.Q_)


    def decode(self, Theta):
        """ Decodes the given knowledge into predicted
        test results.

        Parameters
        ----------
        Theta: ndarray
            A matrix with one row per student and one column per
            skill, where Theta[i, k] represents the estimated
            knowledge of student i for skill k.

        Returns
        -------
        Y: ndarray
            A matrix of predicted test responses for each student
            on each item.

        """
        # multiply Theta with Q to get the relevant knowledge for
        # each item
        Theta_hat = np.dot(Theta, self.Q_.T)
        # subtract the difficulties
        Y = Theta_hat - np.expand_dims(self.b_, 0)
        # binarize result
        Y[Y <= 0.] = 0.
        Y[Y > 0.]  = 1.
        return Y


    def decode_proba(self, Theta):
        """ Decodes the given knowledge into success probabilities.

        Parameters
        ----------
        Theta: ndarray
            A matrix with one row per student and one column per
            skill, where Theta[i, k] represents the estimated
            knowledge of student i for skill k.

        Returns
        -------
        P: ndarray
            A matrix of predicted success probabilities for each
            student on each item.

        """
        # multiply Theta with Q.T to get the relevant knowledge for
        # each item
        Theta_hat = np.dot(Theta, self.Q_.T)
        # subtract the difficulties
        Y = Theta_hat - np.expand_dims(self.b_, 0)
        # multiply with discrimination parameter
        Y = Y * np.expand_dims(self.a_, 0)
        # apply logistic function
        return 1. / (1. + np.exp(-Y))


    def predict(self, X):
        """ Auto-encodes the given test results.

        Parameters
        ----------
        X: ndarray
            A matrix of test responses where each row represents
            a student and each column represents a question.

        Returns
        -------
        Y: ndarray
            A matrix of predicted test responses for each student
            on each item.

        """
        Theta = self.encode(X)
        return self.decode(Theta)


    def predict_proba(self, X):
        """ Auto-encodes the given test results.

        Parameters
        ----------
        X: ndarray
            A matrix of test responses where each row represents
            a student and each column represents a question.

        Returns
        -------
        P: ndarray
            A matrix of predicted success probabilities for each
            student on each item.

        """
        Theta = self.encode(X)
        return self.decode_proba(Theta)

    def Q(self):
        return self.Q_

    def difficulties(self):
        return self.b_
